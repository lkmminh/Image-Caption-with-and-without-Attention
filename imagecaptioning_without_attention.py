# -*- coding: utf-8 -*-
"""ImageCaptioning_Without_Attention.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HEFWH-LBERKRzbBZUFwQmbl-5H5b0ftw

# Without Attention
"""
# hello
class EncoderCNN(nn.Module):
    def __init__(self,embed_size):
        super(EncoderCNN,self).__init__()
        resnet = models.resnet50(pretrained=True)
        for param in resnet.parameters():
            param.requires_grad_(False)

        modules = list(resnet.children())[:-1]
        self.resnet = nn.Sequential(*modules)
        self.embed = nn.Linear(resnet.fc.in_features,embed_size)
        #torch.Size([4, 400])
    def forward(self,images):
        features = self.resnet(images) #torch.Size([batch= 4, chiều đặc trưng = 2048, 1, 1])
        features = features.view(features.size(0),-1)
        features = self.embed(features)
        return features #torch.Size([4, 400])

class DecoderRNN(nn.Module):
    def __init__(self,embed_size,hidden_size,vocab_size,num_layers=1,drop_prob=0.3):
        super(DecoderRNN,self).__init__()
        self.embedding = nn.Embedding(vocab_size,embed_size) # biểu diễn từ thành vecto
        self.lstm = nn.LSTM(embed_size,hidden_size,num_layers=num_layers,batch_first=True) #xử lý chuỗi đầu vào
        self.fcn = nn.Linear(hidden_size,vocab_size) #dự đoán từ tiếp theo
        self.drop = nn.Dropout(drop_prob)

    def forward(self,features, captions):
        embeds = self.embedding(captions[:,:-1]) #biểu diễn từ thành embedding
        #torch.Size([4, 14, 400])
        x = torch.cat((features.unsqueeze(1),embeds),dim=1) #nối features và embedding lại
        #torch.Size([4, 15, 400]) [batch_size, seq_len+1, embed_size]
        x,_ = self.lstm(x) #torch.Size([4, 15, 512])[batch_size, seq_len+1, hidden_size]
        x = self.fcn(x) #torch.Size([4, 15, 2994(len(vocab)])
        return x

    def generate_caption(self,inputs,hidden=None,max_len=20,vocab=None):
        batch_size = inputs.size(0) #đặc trưng hình ảnh

        captions = []
        #torch.Size([4, 14])
        #dự đoán chú thích
        for i in range(max_len):#lặp qua mỗi từ
            output,hidden = self.lstm(inputs,hidden) #sử dụng trạng thái ẩn trước đó của LSTM để dự đoán
            output = self.fcn(output)
            output = output.view(batch_size,-1)

            predicted_word_idx = output.argmax(dim=1)

            captions.append(predicted_word_idx.item())

            if vocab.itos[predicted_word_idx.item()] == "<EOS>":
                break

            inputs = self.embedding(predicted_word_idx.unsqueeze(0))

        return [vocab.itos[idx] for idx in captions]


class EncoderDecoder(nn.Module):
    def __init__(self,embed_size,hidden_size,vocab_size,num_layers=1,drop_prob=0.3):
        super(EncoderDecoder,self).__init__()
        self.encoder = EncoderCNN(embed_size)#mã hóa đặc trưng
        self.decoder = DecoderRNN(embed_size,hidden_size,vocab_size,num_layers,drop_prob)
        #giải mã các đặc trưng
    def forward(self, images, captions):
        features = self.encoder(images)
        outputs = self.decoder(features, captions)
        return outputs
# resenet features shape - torch.Size([4, 2048, 1, 1])
# resenet features viewed shape - torch.Size([4, 2048])
# resenet features embed shape - torch.Size([4, 400])
# caption shape - torch.Size([4, 14])
# shape of embeds - torch.Size([4, 14, 400])
# features shape - torch.Size([4, 400])
# features unsqueeze at index 1 shape - torch.Size([4, 1, 400])
# shape of x - torch.Size([4, 15, 400])
# shape of x after lstm - torch.Size([4, 15, 512])
# shape of x after fcn - torch.Size([4, 15, 2994])

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

# Hyperparameters
embed_size = 400
hidden_size = 512
vocab_size = len(dataset.vocab)
num_layers = 2
learning_rate = 0.0001
num_epochs = 2

# initialize model, loss etc
model = EncoderDecoder(embed_size, hidden_size, vocab_size, num_layers).to(device)
criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi["<PAD>"])
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

num_epochs = 20
print_every = 2000

for epoch in range(1,num_epochs+1):
    for idx, (image, captions) in enumerate(iter(data_loader)):
        image,captions = image.to(device),captions.to(device)

        # Zero the gradients.
        optimizer.zero_grad()

        # Feed forward
        outputs = model(image, captions)

        # Calculate the batch loss.
        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))


        # Backward pass.
        loss.backward()

        # Update the parameters in the optimizer.
        optimizer.step()

        if (idx+1)%print_every == 0:
            print("Epoch: {} loss: {:.5f}".format(epoch,loss.item()))


            #generate the caption
            model.eval()
            with torch.no_grad():
                dataiter = iter(data_loader)
                img,_ = next(dataiter)
                features = model.encoder(img[0:1].to(device))
                print(f"features shape - {features.shape}")
                caps = model.decoder.generate_caption(features.unsqueeze(0),vocab=dataset.vocab)
                caption = ' '.join(caps)
                print(caption)
                show_image(img[0],title=caption)

            model.train()